# Architecture Globale OpenShift AI 2.22

## üèóÔ∏è Vue d'ensemble

OpenShift AI 2.22 est construit sur une architecture modulaire utilisant un meta-operator qui d√©ploie et g√®re tous les composants via un objet DataScienceCluster sur OpenShift Container Platform 4.19.

## üìä Diagramme d'architecture

üëâ **[Voir le diagramme interactif complet](./index.html)**

```mermaid
graph TB
    subgraph "üèóÔ∏è OpenShift Container Platform 4.19"
        K8S[Kubernetes API Server<br/>etcd ‚Ä¢ CRI-O ‚Ä¢ Kubelet]
        OCP[OpenShift Core<br/>OAuth ‚Ä¢ RBAC ‚Ä¢ Routes]
        OLM[Operator Lifecycle Manager<br/>OperatorHub]
    end
    
    subgraph "üéõÔ∏è Red Hat OpenShift AI Operator Meta-Operator"
        DSC[DataScienceCluster DSC<br/>Gestion composants<br/>managementState: Managed/Removed]
        DSCI[DSCInitialization DSCI<br/>Configuration globale<br/>Namespaces]
    end
    
    subgraph "üìÅ Namespaces OpenShift AI"
        NS1[redhat-ods-operator<br/>Op√©rateur principal]
        NS2[redhat-ods-applications<br/>Composants applicatifs]
        NS3[redhat-ods-monitoring<br/>Monitoring et m√©triques]
        NS4[rhods-notebooks<br/>Workbenches utilisateur]
    end
    
    subgraph "üß© Composants Core"
        DASH[Dashboard<br/>Interface web<br/>Gestion projets/utilisateurs<br/>managementState: Managed]
        WB[Workbenches<br/>JupyterLab<br/>Notebooks personnalis√©s<br/>managementState: Managed]
        MR[Model Registry<br/>Gestion mod√®les<br/>üÜï OAuth Proxy 2.22<br/>managementState: Managed]
    end
    
    subgraph "üöÄ Model Serving"
        KS[KServe<br/>Single-Model Serving<br/>LLMs ‚Ä¢ GPU autoscaling<br/>Serverless/RawDeployment<br/>managementState: Managed]
        MM[ModelMesh<br/>Multi-Model Serving<br/>Ressources partag√©es<br/>managementState: Managed]
    end
    
    subgraph "‚öôÔ∏è MLOps & Pipelines"
        DSP[Data Science Pipelines<br/>Kubeflow Pipelines 2.5.0<br/>Workflows Docker<br/>managementState: Managed]
        TA[TrustyAI<br/>Monitoring mod√®les<br/>Explicabilit√© IA<br/>managementState: Managed]
    end
    
    subgraph "üñ•Ô∏è Distributed Workloads"
        CF[CodeFlare<br/>SDK Python<br/>Orchestration<br/>managementState: Managed]
        RAY[Ray<br/>Calcul distribu√©<br/>Parallel processing<br/>managementState: Managed]
        KUEUE[Kueue<br/>Gestion queues<br/>Ressources partag√©es<br/>managementState: Managed]
        TO[Training Operator<br/>Entra√Ænement ML<br/>Jobs distribu√©s<br/>managementState: Managed]
    end
    
    subgraph "üîó Op√©rateurs D√©pendants Externes"
        SM[OpenShift Service Mesh<br/>Istio Control Plane<br/>Requis pour KServe Advanced]
        SL[OpenShift Serverless<br/>Knative Serving/Eventing<br/>Requis pour KServe Serverless]
        AUTH[Authorino Optionnel<br/>Authentification mod√®les<br/>Token validation]
        GPU[NVIDIA GPU Operator<br/>Node Feature Discovery<br/>Support GPU]
    end
    
    subgraph "üíæ Stockage & D√©pendances"
        S3[Stockage S3-Compatible<br/>AWS S3 ‚Ä¢ MinIO ‚Ä¢ Ceph<br/>IBM Cloud Storage<br/>Requis pour mod√®les]
    end
    
    %% Relations principales
    K8S --> DSC
    OCP --> DSC
    OLM --> DSC
    
    DSC --> NS1
    DSC --> NS2
    DSC --> NS3
    DSC --> NS4
    
    DSC --> DASH
    DSC --> WB
    DSC --> MR
    DSC --> KS
    DSC --> MM
    DSC --> DSP
    DSC --> TA
    DSC --> CF
    DSC --> RAY
    DSC --> KUEUE
    DSC --> TO
    
    %% D√©pendances externes
    KS --> SM
    KS --> SL
    KS --> AUTH
    WB --> GPU
    TO --> GPU
    
    %% Stockage
    KS --> S3
    MM --> S3
    DSP --> S3
    MR --> S3
    
    %% Styles
    classDef platform fill:#2c3e50,stroke:#34495e,color:#fff
    classDef operator fill:#e74c3c,stroke:#c0392b,color:#fff
    classDef core fill:#3498db,stroke:#2980b9,color:#fff
    classDef serving fill:#00b894,stroke:#00a085,color:#fff
    classDef mlops fill:#9b59b6,stroke:#8e44ad,color:#fff
    classDef workload fill:#f39c12,stroke:#e67e22,color:#fff
    classDef dependency fill:#95a5a6,stroke:#7f8c8d,color:#fff
    classDef storage fill:#27ae60,stroke:#229954,color:#fff
    classDef namespace fill:#e8f4f8,stroke:#3498db,color:#2c3e50
    
    class K8S,OCP,OLM platform
    class DSC,DSCI operator
    class NS1,NS2,NS3,NS4 namespace
    class DASH,WB,MR core
    class KS,MM serving
    class DSP,TA mlops
    class CF,RAY,KUEUE,TO workload
    class SM,SL,AUTH,GPU dependency
    class S3 storage
```

## üéõÔ∏è Meta-Operator : Red Hat OpenShift AI Operator

**D√©finition officielle** : *Un m√©ta-op√©rateur qui d√©ploie et maintient tous les composants et sous-op√©rateurs qui font partie d'OpenShift AI.*

### DataScienceCluster (DSC)
L'objet principal qui configure tous les composants OpenShift AI :

```yaml
apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
spec:
  components:
    dashboard:
      managementState: Managed
    workbenches:
      managementState: Managed
    kserve:
      managementState: Managed
    modelmeshserving:
      managementState: Managed
    datasciencepipelines:
      managementState: Managed
    # ... autres composants
```

### Configuration managementState
- **Managed** : L'op√©rateur g√®re activement le composant, l'installe, et essaie de le maintenir actif
- **Removed** : L'op√©rateur g√®re activement le composant mais ne l'installe pas. Si d√©j√† install√©, l'op√©rateur essaiera de le supprimer

## üìÅ Namespaces cr√©√©s automatiquement

Lors de l'installation de l'op√©rateur, les namespaces suivants sont cr√©√©s :

- **`redhat-ods-operator`** : Contient l'op√©rateur Red Hat OpenShift AI
- **`redhat-ods-applications`** : Installe le dashboard et autres composants requis
- **`redhat-ods-monitoring`** : Contient les services de monitoring et facturation
- **`rhods-notebooks`** : O√π les environnements de notebooks sont d√©ploy√©s par d√©faut

## üß© Composants Core

### Dashboard
*Un dashboard orient√© client qui montre les applications disponibles et install√©es pour l'environnement OpenShift AI ainsi que les ressources d'apprentissage comme les tutoriels, guides de d√©marrage rapide et documentation.*

**Fonctionnalit√©s** :
- Interface web utilisateur
- Gestion des projets et utilisateurs
- Configuration des images de notebooks
- Gestion des profils d'acc√©l√©rateurs GPU
- Administration des runtimes de serving

### Workbenches
*Une application auto-g√©r√©e qui permet aux data scientists de configurer leur propre environnement de serveur de notebooks et d√©velopper des mod√®les de machine learning dans JupyterLab.*

**Caract√©ristiques** :
- Environnements JupyterLab personnalisables
- Images de base configurables
- Int√©gration avec stockage persistant
- Support GPU optionnel

### Model Registry (üÜï Nouveaut√© 2.22)
*Les op√©rateurs Red Hat Authorino, Red Hat OpenShift Serverless, et Red Hat OpenShift Service Mesh ne sont plus requis pour utiliser le composant model registry dans OpenShift AI.*

**Am√©liorations 2.22** :
- OAuth Proxy int√©gr√© remplace Authorino
- Migration automatique des instances existantes
- Simplification de l'installation
- Nouvelles instances utilisent OAuth proxy par d√©faut

## üöÄ Model Serving Components

### KServe (Single-Model Serving)
*Pour les mod√®les volumineux, comme les grands mod√®les de langage, la plateforme Single-Model Serving utilise KServe, d√©ployant chaque mod√®le sur son propre serveur.*

**Modes de d√©ploiement** :
- **Serverless** : Utilise Knative pour autoscaling
- **RawDeployment** : D√©ploiement standard Kubernetes

**D√©pendances** :
- Red Hat OpenShift Serverless (pour mode Serverless)
- Red Hat OpenShift Service Mesh (pour mode Advanced)
- Authorino (optionnel, pour authentification)

### ModelMesh (Multi-Model Serving)
*Pour les mod√®les petits et moyens, la plateforme Multi-Model Serving utilise ModelMesh pour d√©ployer plusieurs mod√®les sur un serveur unique, partageant les ressources efficacement.*

**Avantages** :
- Optimisation des ressources
- Partage efficace des co√ªts
- Gestion centralis√©e
- Surveillance unifi√©e

## ‚öôÔ∏è MLOps & Pipeline Components

### Data Science Pipelines
*Les data scientists peuvent construire des workflows de machine learning (ML) portables avec data science pipelines 2.0, utilisant des conteneurs Docker.*

**Mise √† jour 2.22** : Kubeflow Pipelines version 2.5.0

**Fonctionnalit√©s** :
- Workflows automatis√©s
- Conteneurs Docker
- Orchestration des √©tapes ML
- Int√©gration avec stockage S3

### TrustyAI
**Fonctionnalit√©s** :
- Monitoring des mod√®les en production
- Explicabilit√© de l'IA
- D√©tection de biais
- M√©triques de performance

## üñ•Ô∏è Distributed Workloads

*Les data scientists peuvent utiliser plusieurs n≈ìuds en parall√®le pour entra√Æner des mod√®les de machine learning ou traiter des donn√©es plus rapidement.*

### CodeFlare
- SDK Python pour orchestration distribu√©e
- Interface simplifi√©e pour Ray
- Gestion des ressources automatis√©e

### Ray
- Calcul distribu√© et parallel processing
- Mise √† l'√©chelle automatique
- Support pour machine learning distribu√©

### Kueue
- Gestion des queues de travail
- Planification intelligente des ressources
- Partage √©quitable des ressources

### Training Operator
- Entra√Ænement ML distribu√©
- Support pour jobs parall√®les
- Optimisation multi-GPU

## üîó D√©pendances Externes

### Op√©rateurs requis pour KServe
- **Red Hat OpenShift Serverless** : Knative Serving/Eventing
- **Red Hat OpenShift Service Mesh** : Istio Control Plane
- **Red Hat - Authorino** (optionnel) : Authentification des mod√®les

### Stockage
**Requis** : Stockage compatible S3 (AWS S3, MinIO, Ceph, IBM Cloud Storage)
**Utilisation** :
- Stockage des mod√®les ML
- Artifacts des pipelines
- Donn√©es d'entra√Ænement
- Logs et m√©triques

### Support GPU (optionnel)
- **NVIDIA GPU Operator**
- **Node Feature Discovery Operator**

## üÜï Nouveaut√©s principales OpenShift AI 2.22

### Simplification du Model Registry
- **Avant** : N√©cessitait Authorino + Serverless + Service Mesh
- **Maintenant** : OAuth Proxy int√©gr√© uniquement
- **Migration** : Automatique pour les instances existantes

### Am√©lioration de la r√©silience
- **Op√©rateur** : Maintenant avec 3 r√©pliques
- **Haute disponibilit√©** : Am√©lioration de la r√©silience pour les workloads de production
- **Distribution des webhooks** : Op√©rations r√©parties sur plusieurs instances

### Mise √† niveau des pipelines
- **Kubeflow Pipelines 2.5.0** : Nouvelles fonctionnalit√©s et corrections
- **Compatibilit√© am√©lior√©e** : Meilleure int√©gration avec l'√©cosyst√®me

## üìö Documentation officielle

### Sources principales
- [Architecture d'OpenShift AI Cloud Service](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service/architecture-of-openshift-ai_install)
- [Architecture OpenShift AI Self-Managed](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/installing_and_uninstalling_openshift_ai_self-managed/architecture-of-openshift-ai-self-managed_install)
- [Installation et d√©ploiement](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/installing_and_uninstalling_openshift_ai_cloud_service/installing-and-deploying-openshift-ai_install)
- [Serving Models](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html-single/serving_models/index)

### Nouveaut√©s et releases
- [Nouvelles fonctionnalit√©s 2.22](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/release_notes/new-features-and-enhancements_relnotes)
- [Issues r√©solues](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/release_notes/resolved-issues_relnotes)

## üîß Installation et configuration

### Pr√©requis
1. **OpenShift Container Platform 4.19+**
2. **Privil√®ges cluster administrator**
3. **Stockage par d√©faut** configur√© et provisionnement dynamique
4. **Pour KServe** : Installation des op√©rateurs Serverless et Service Mesh

### √âtapes d'installation
1. **Installer l'op√©rateur** depuis OperatorHub
2. **Cr√©er DataScienceCluster** avec composants souhait√©s
3. **Configurer le stockage** S3-compatible
4. **Activer GPU** si n√©cessaire
5. **Configurer l'authentification** selon les besoins

---

**Bas√© sur** : Documentation officielle Red Hat OpenShift AI 2.22  
**Compatible** : OpenShift Container Platform 4.19+
